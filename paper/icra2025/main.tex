\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Enhancing Depth Estimation with Iterative Stereo-LiDAR Fusion: An Application of the Papoulis-Gerchberg Algorithm\\
{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
should not be used}
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{
\IEEEauthorblockN{1\textsuperscript{st} Assc. Prof. İsmail Uyanık}
\IEEEauthorblockA{\textit{Department of Electrical and Electronics Engineering} \\
\textit{Hacettepe University}\\
Ankara, Turkiye \\
uyanik@ee.hacettepe.edu.tr}
\and
\IEEEauthorblockN{2\textsuperscript{st} Ahmet Yusuf Şirin}
\IEEEauthorblockA{\textit{Department of Electrical and Electronics Engineering} \\
\textit{Hacettepe University}\\
Ankara, Turkiye \\
ayusufsirin@gmail.com}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Ertuğrul Tiyek}
\IEEEauthorblockA{\textit{Department of Electrical and Electronics Engineering} \\
\textit{Hacettepe University}\\
Ankara, Turkiye \\
ertugrultyk@gmail.com}
}

\maketitle

\begin{abstract}
Depth perception in autonomous systems is critically enhanced by sensor fusion techniques, which traditionally rely on the combination of stereo vision and LiDAR data. However, the challenge of integrating sparse LiDAR data with dense stereo imagery remains. This paper introduces a novel fusion approach that applies the Papoulis-Gerchberg algorithm to iteratively refine stereo depth maps with LiDAR data. Our method uses LiDAR measurements as anchor points to guide the enhancement of depth information, resulting in superior resolution and accuracy. We evaluate our approach using the KITTI benchmark suite, demonstrating significant improvements over existing methods such as FastFusion, particularly in edge definition and object boundary sharpness. The results suggest our method not only achieves higher fidelity in depth estimation but also offers a new perspective on sensor data integration for real-time applications in autonomous navigation.
\end{abstract}

\begin{IEEEkeywords}
Sensor Fusion, Stereo Vision, LiDAR Depth Sensing, Papoulis-Gerchberg Algorithm, Autonomous Vehicles, Iterative Enhancement, Depth Map, Super-Resolution, Real-Time Depth Estimation
\end{IEEEkeywords}

\section{Introduction}

The quest for accurate depth perception in autonomous systems has given rise to various sensor fusion methodologies, predominantly those employing neural networks. While these data-driven techniques have demonstrated remarkable performance, they often require extensive training on large datasets and substantial computational resources, limiting their practicality in dynamic real-world scenarios.

Contrary to the prevalent neural network-based approaches, this paper introduces a novel sensor fusion framework that does not rely on learning from data. By adapting the Papoulis-Gerchberg algorithm, traditionally used for signal reconstruction and super-resolution, our method iteratively refines stereo depth maps using LiDAR data without the need for training. This non-learning-based approach circumvents the complexities of neural models, resulting in a lightweight and versatile solution suitable for real-time applications.

Our methodology capitalizes on the complementary strengths of stereo vision and LiDAR, embedding accurate depth measurements into dense stereo maps to produce superior environmental representations. By eschewing neural networks, we avoid their inherent demand for computational power and data, presenting an approach that is not only more practical but also readily deployable in diverse operational contexts.

The following sections will outline the current landscape of depth sensing and fusion techniques, detail our fusion algorithm, and present a comparative analysis with existing neural network-based methods, showcasing the practical and performance benefits of our approach.

\section{Related Works}

Stereo matching and depth estimation are pivotal in applications such as autonomous driving and robotic navigation. Traditional methods, as outlined by Scharstein and Szeliski (2002), have evolved with the adoption of Deep Neural Network (DNN) techniques, significantly enhancing accuracy (Zbontar \& LeCun, 2016). However, the computational demands of these DNN solutions often outpace the capabilities of onboard systems in real-time scenarios (Shamsafar et al., 2022; Rahim et al., 2021).

In the realm of stereo-LiDAR fusion, approaches like those proposed by Parks et al. (2018) and Park et al. (2019) augment stereo disparity with LiDAR data but fail to fully exploit the precision of LiDAR. Choe et al. (2021) and Yang et al. (2019) introduce architectures that incorporate LiDAR data into early stages of depth estimation but often at a sacrifice of either depth integration or accuracy. Other methods, such as those by Mai et al. (2021) and Li and Cao (2020), achieve higher accuracy but at the cost of increased computational load, which is not conducive to real-time application.

Our work diverges from these DNN-heavy strategies by employing the Papoulis-Gerchberg algorithm, a technique historically utilized in signal processing for super-resolution, adapted here for sensor fusion. This approach circumvents the prohibitive computational demands of DNNs, offering a lightweight yet robust alternative for real-time, accurate depth estimation from stereo-LiDAR data fusion.

\section{Technical Approach}
%% In this section explain each step of the processing detaily.

The Papoulis-Gerchberg (PG) algorithm is an iterative technique employed in signal processing to perform band-limited signal extrapolation. This method enables the estimation of missing portions of a signal based on a limited segment of the original data and the knowledge of its bandwidth.

The PG algorithm operates through an iterative process involving two key steps:

\begin{enumerate}
    \item Constraint Enforcement in the Spatial Domain:
    The initial step involves enforcing the known samples of the signal. This is achieved by transforming the estimated spectrum (obtained through techniques like the Fourier transform) of the signal and modifying it to ensure it aligns with the known Fourier Transform values corresponding to the available samples.
    
    \item Constraint Enforcement in the Frequency Domain:
    The second step focuses on enforcing the known bandwidth constraint. This is implemented by modifying the signal's estimated version in the spatial domain (e.g., time domain for time-based signals) to guarantee it adheres to the specified bandwidth limitations.
\end{enumerate}

These two steps are repeated iteratively. As each iteration progresses, the estimated signal progressively converges towards the original signal, ultimately resulting in an extrapolated version that replenishes the missing data points.

Further works opened the papoulis-gerchberg algorithm new using areas like signal restoration and image processing.
%% Our approach is based on the PG method that is commonly used for estimation and reconstruction of lossy signals in communications systems. The method is applicable for 1D signals, originally. The 2D implementation of it is required for us. There are some works on implementation to 2D of PG. 

%% Paper about signal restoration
%% Paper about super resolution
An application of this algorithm for image inpainting was proposed by \cite{pg-image-inpaint-2009}, wherein the Papoulis-Gerchberg (PG) algorithm is adapted for this particular scenario. However, there are several caveats to its application within our context. Primarily, the images utilized in their study exhibit significantly higher density compared to LiDAR data. Furthermore, their methodology is predicated upon the utilization of a singular sensory modality, whereas our objective necessitates the achievement of fusion across multiple sensors.

Another noteworthy contribution is made by \cite{ozbay2015high}, presenting an enhanced iteration of the PG algorithm tailored for 3D point cloud data derived from LiDAR sensors. Their method focuses on upsampling to mitigate the latency in point acquisition observed when operating LiDAR at high-resolution modes, attributable to the inherently slow scan rates of cylindrical scanning systems. Nevertheless, it is important to note that this approach, too, relies on a singular sensory system.

The utilization of the Papoulis-Gerchberg (PG) algorithm within our framework is primarily motivated by its deterministic nature, setting it apart from DNN-based methodologies that often rely on probabilistic outcomes. A closer examination of the PG algorithm's underlying principles reveals two pivotal components: a densely populated base image intended for gap-filling using points of known accuracy, and the employment of these high-confidence points for iterative refinement. This paradigm bears a striking resemblance to the challenges inherent in sensor fusion. In our methodology, stereo depth images serve as the densely populated base analogous to the PG algorithm's framework, while LiDAR sparse data represent the high-confidence points. This analogy forms the cornerstone of our approach, leveraging the PG algorithm's architecture to integrate multi-sensorial data, thereby pioneering a novel application of PG in the realm of sensor fusion.

\subsection{Pre-Processing}

The fusion of data from heterogeneous sensor systems, such as LiDAR and stereo cameras, presents a unique set of challenges, primarily due to the fundamental differences in their respective depth information representations. LiDAR sensors provide sparse but highly accurate point clouds, whereas stereo vision techniques yield dense disparity maps with varying degrees of precision. To effectively integrate these diverse data modalities, a comprehensive pre-processing regimen is indispensable. This regimen encompasses transformation, conversion, and upsampling procedures designed to standardize the data, thereby laying the groundwork for their combined analysis.

Transformation and Conversion: The initial step involves transforming the LiDAR-generated point clouds and stereo vision-produced disparity maps into a uniform format. We opt for the single-channel depth image format as this common ground facilitates the merging process. This transformation is critical not only for aligning the data spatially but also for ensuring that subsequent processing stages operate on data that are consistent in terms of dimensionality and scale.

Upsampling: Given the sparse nature of LiDAR data compared to the relatively denser outputs from stereo vision, upsampling is employed to increase the resolution of the LiDAR measurements. This step is crucial for achieving a density comparable to that of the stereo disparity maps, thereby enhancing the reliability of the fused depth information. Upsampling is carefully executed to preserve the integrity of the LiDAR data, ensuring that the enhanced resolution does not compromise the accuracy inherent to LiDAR measurements.

RGB Integration Exclusion: It is worth noting that our approach does not necessitate the integration of RGB data. This decision streamlines the preprocessing phase, focusing solely on depth information, which is paramount for our application. The exclusion of RGB data underscores our methodology's emphasis on depth accuracy and computational efficiency, aligning with the objective of developing a lightweight and practical fusion algorithm.

By meticulously addressing these preprocessing steps, we ensure that the LiDAR and stereo depth data are optimally prepared for fusion. This preparation is vital for the successful application of our novel algorithm, which leverages the strengths of both sensor types to produce depth estimations of unparalleled accuracy and density.

\subsubsection{Obtaining Depth Frame From LiDAR Data}

Subsequent to the calibration of the stereo camera and LiDAR sensor, the transformation matrices essential for converting point cloud data into a depth image are established. The conversion employs the pinhole camera model to accurately project three-dimensional points onto a two-dimensional depth frame space. This process is fundamental in translating the spatial coordinates of LiDAR measurements into a format that is compatible with stereo vision data, thereby facilitating their integration.

\subsubsection{Data Cropping}

Projection of LiDAR data onto the stereo depth image plane reveals discrepancies in the boundary extents of the depth regions. This is particularly evident in the vertical scan range of the LiDAR sensor, which is relatively narrow (approximately 30 degrees) compared to the broader field of view captured by the stereo camera. Consequently, it becomes necessary to crop the stereo depth image to match the LiDAR's scan area. Moreover, the stereo depth image exhibits a narrower horizontal boundary compared to the LiDAR's 360-degree scanning capability. Given our algorithm's reliance on depth images and the absence of memory for previously detected points in frame sensor fusion, non-overlapping depth data regions are disregarded to maintain algorithmic integrity and focus on the fusion of congruent depth information.

\subsubsection{Data Augmentation (Stereo Infill)}

The augmentation of stereo depth data addresses the challenge posed by inherent limitations in stereo vision disparity maps, which frequently exhibit null values in occluded regions not visible to one or both of the stereo cameras. These occlusions result in incomplete depth information, manifesting as gaps in the depth images derived from stereo vision. To rectify this and ensure a comprehensive depth field, our method incorporates a strategic process of gap filling that leverages high-confidence LiDAR measurements.

Gap Segmentation and Identification: The initial phase involves a meticulous analysis of the stereo depth images to segment and identify areas devoid of depth information—these are the regions where disparity maps fail to provide reliable depth due to occlusions or insufficient texture.

LiDAR Measurement Integration: Upon identifying these gaps, the process proceeds by overlaying LiDAR data onto the stereo depth images. This step utilizes the precise depth information from LiDAR measurements to fill in the identified gaps, prioritizing regions where LiDAR data overlaps with the gaps in stereo images. This integration capitalizes on the accuracy and reliability of LiDAR measurements to enhance the fidelity of the depth information in areas previously marked by uncertainty.

Interpolation for Comprehensive Coverage: Recognizing the sparse nature of LiDAR data and its inability to cover all gaps fully, we employ interpolation techniques as a subsequent measure. This approach extrapolates the available LiDAR measurements to fill larger gaps, ensuring that even areas without direct LiDAR coverage receive enhanced depth estimations. The interpolation considers the spatial distribution of LiDAR points and the surrounding depth context to generate a coherent depth field that bridges the disparity-induced gaps with plausible depth values.

This augmented approach to stereo depth infill not only rectifies the limitations inherent in stereo disparity maps but also enhances the overall depth accuracy of the fused stereo-LiDAR image. By meticulously integrating and interpolating LiDAR data within the stereo depth framework, we achieve a more robust and complete representation of the scene's depth, paving the way for improved performance in subsequent processing stages of our sensor fusion algorithm.

This phase represents a pivotal component of our preprocessing strategy, primarily due to its critical role in safeguarding the integrity of the Papoulis-Gerchberg (PG) algorithm's iterative process. In the absence of this meticulous data augmentation step, the iterative nature of the PG algorithm could inadvertently exacerbate depth estimation errors with each subsequent iteration. Specifically, unaddressed gaps in the stereo depth images could lead to erroneous propagation and amplification of inaccuracies, undermining the algorithm's ability to refine and converge towards a precise depth map. Therefore, by ensuring that these gaps are effectively filled through the integration and interpolation of LiDAR data, we substantially enhance the stability and accuracy of the depth estimation process, particularly as the number of iterations increases. This augmentation is not merely a preparatory measure but a fundamental prerequisite for harnessing the full potential of the PG algorithm in achieving high-fidelity depth estimations from fused stereo-LiDAR data.

\subsubsection{LiDAR Null Mask} \label{lidar-null-mask}

The projection of sparse LiDAR point cloud data onto a two-dimensional plane inherently results in the presence of null values within the depth frames. These null values must be meticulously accounted for prior to the integration of LiDAR depth maps into our fusion algorithm. To address this, we employ a null value mask for the LiDAR depth data, which serves to identify and subsequently exclude spurious depth entities from the fused depth estimation. This preprocessing step is crucial for refining the depth information derived from stereo depth maps, utilizing the LiDAR data as a corrective measure to enhance the fidelity of the resultant depth image.

\subsection{PG: Papoulis Gerchberg}

Initially, our approach sought to leverage the Papoulis-Gerchberg algorithm as a method for pre-processing, specifically aimed at upsampling LiDAR data exclusively. However, the intrinsic sparsity of LiDAR measurements presented substantial challenges. The iterative nature of the Papoulis-Gerchberg method, when applied to areas devoid of data—filled with zero-padding—resulted in an excessively high number of iterations before achieving satisfactory outcomes. Moreover, the sparse nature of the data, particularly along the vertical axis, led to inadequate enhancement in regions distant from the original LiDAR samples, in stark contrast to areas proximal to these samples.

In an effort to mitigate the iteration count, we experimented with initializing the empty regions with the mean depth value derived from the actual LiDAR samples. This approach, leveraging a more representative initial state, markedly reduced the number of necessary iterations by providing a closer approximation to the true depth values of the surrounding area. Nonetheless, the fundamental issue posed by the sparsity of the LiDAR data persisted.
%% add an image to represent the papoulis with zero-padding

Addressing this challenge required a more robust solution. We turned to the work of our colleague, Ismail Uyanik, specifically his pioneering method, "Enhancing 3D range image measurement density via the dynamic Papoulis-Gerchberg algorithm." This approach, coupled with our mean value initialization strategy, offered a comprehensive solution. By incorporating a dynamic iteration of the PG algorithm, we significantly enhanced our ability to refine the depth data, particularly in sparsely measured areas, thereby overcoming the limitations previously encountered with LiDAR data sparsity.
%% add another figure for mean filling

% We need more significant data to overcome this problem. The initial solution was the using previous work of our colleague @ismail-uyanik, {Enhancing 3D range image measurement density via dynamic Papoulis-Gerchberg algorithm} along with the mean filling method.

% BU DA OLABİLİR. İKİSİNİ DE GPT YAZDI
% Initially, our strategy entailed leveraging the Papoulis-Gerchberg (PG) algorithm predominantly as a pre-processing technique to upscale LiDAR data exclusively. However, the inherent sparsity of LiDAR measurements posed significant challenges, particularly in maintaining the fidelity of depth information across the depth frame.

% In an attempt to address the sparse nature of the LiDAR data, we explored the application of zero-padding to the vacant regions within the depth frames derived from LiDAR samples. Despite the iterative prowess of the PG algorithm, the introduction of a zero-value background substantially increased the number of iterations required to achieve satisfactory outcomes. More critically, due to the pronounced sparsity along the vertical axis, areas distant from the original LiDAR points consistently yielded inferior results compared to those in closer proximity.

% To mitigate the excessive iteration counts, an alternative initialization strategy was considered, involving the use of the mean depth value from actual LiDAR samples as a background filler. This approach, by aligning the initial background estimation closer to the real depth values of neighboring points, markedly reduced the necessary iterations for convergence. Nonetheless, the fundamental issue of data sparsity remained inadequately addressed.

% The breakthrough came with the incorporation of a technique developed by our colleague, Ismail Uyanik, titled "Enhancing 3D Range Image Measurement Density via Dynamic Papoulis-Gerchberg Algorithm." By integrating this method with our mean filling approach, we were able to significantly augment the density of the depth data. This synergy not only addressed the sparsity challenge but also empowered the PG algorithm to reconstruct depth frames with enhanced accuracy and reduced computational overhead, laying a robust foundation for the subsequent fusion process.

%% cite the paper from kuzucu-elvan
%% add another image explaining the plan
\paragraph{Our Approach}

The primary objective of this research is to explore the integration of depth information derived from stereo cameras and LiDAR measurements. Our approach innovatively combines the upsampling of LiDAR data with the fusion of depth information from both sources. This methodology significantly reduces the computational demands, enabling the real-time application of sensor fusion on mobile computing platforms.

At the core of our method is the strategy of utilizing the depth frame obtained from the stereo image pair as the background for LiDAR measurements, rather than relying solely on a mean value for background estimation. This approach effectively addresses the issue of LiDAR data sparsity by providing a rich, relevant context for the depth information. Consequently, this significantly enhances the depth frame's completeness and accuracy.

Furthermore, we have adapted and refined the traditional 2D Papoulis-Gerchberg algorithm to better suit the unique requirements of our sensor fusion task.

\paragraph{Use of Band Reject Filter in PG}

A critical modification in our adaptation of the Papoulis-Gerchberg algorithm involves the application of a band-reject filter. The inherent design of low-pass filters within the PG algorithm is to attenuate high-frequency components, under the assumption that a continuous signal does not encompass abrupt transitions. Such transitions are typically interpreted as signal loss, prompting the PG algorithm to attempt image reconstruction by excluding high-frequency details.

However, in the context of depth estimation, these high-frequency components or sharp transitions are indicative of the boundaries between foreground and background elements, carrying essential spatial information that we aim to preserve. To retain this critical detail, we introduce a band-reject filter that selectively allows the passage of both higher and lower frequency components while filtering out mid-frequency ranges. These mid-frequencies are presumed to represent discrepancies between LiDAR measurements and the depth estimates from the stereo pair.

Through iterative application of this filter, coupled with the strategic placement of LiDAR points as accuracy anchors, our method progressively refines the depth estimation, aligning it more closely with the inherent precision of LiDAR data. This tailored filtering approach, therefore, plays a pivotal role in mitigating the fusion process's challenges, ensuring that the transition zones between different depth regions are accurately maintained, thereby enhancing the overall fidelity of the fused depth map.


% \paragraph{Evaluated PG for Our Work}

\subsection{Post-Processing}

The post-processing phase is designed to refine the fused depth data, ensuring its accuracy and consistency. This phase is divided into two primary components:

\subsubsection{Removal of Trivial Augmented Data}

A key step involves utilizing the stereo null mask to identify and eliminate any trivial or less reliable data that was introduced during the data augmentation process. Specifically, depth estimations that fall within areas masked by the stereo null mask are removed from the final depth frame. This mask effectively delineates regions where the stereo data lacked sufficient information, ensuring that only depth data with a high confidence level, either directly measured or accurately estimated, is retained in the final output.

\subsubsection{Enhancement and Smoothing}

To further improve the quality of the fused depth maps, an enhancement and smoothing process is applied. This involves the use of filtering techniques to reduce noise and smooth out rough edges, enhancing the overall visual quality of the depth map. Techniques such as Gaussian smoothing or median filtering can be particularly effective in achieving a more coherent depth representation, especially in areas where the fusion of stereo and LiDAR data may produce discontinuities or artifacts.

\subsubsection{Final Depth Consistency Check}

Another critical component of the post-processing phase is the final depth consistency check. This step involves comparing the processed depth map against the original LiDAR and stereo data to identify and correct any discrepancies. Discrepancy correction can be performed through a combination of automated threshold-based adjustments and manual inspections, ensuring that the final depth map faithfully represents the real-world scene.

\subsubsection{Edge Sharpening}

Lastly, edge sharpening techniques are applied to restore and emphasize the definition of edges that may have been softened during the smoothing process. This is particularly important for maintaining the structural integrity of objects within the scene, allowing for clear delineation between different entities and the background. Techniques such as unsharp masking or adaptive sharpening filters can be employed to enhance edge clarity without introducing noise or artifacts.


To provide a comprehensive explanation of the algorithm's implementation and its adaptation to linear algebra, let's expand on the structure you've outlined, detailing both the building and adaptation phases with clarity and technical depth.

\section{Implementation} \label{implementation}

The successful implementation of our proposed sensor fusion method is paramount, particularly given the stringent performance and efficiency requirements characteristic of robotic applications. These requirements include, but are not limited to, the need for high-frequency processing capabilities, efficient handling of data rates from multiple sensors, and adherence to computational constraints. To navigate these challenges, we harness the computational efficiency of linear algebra and the precision of mathematical formulations.

\subsection{Building the Algorithm}

The construction of our algorithm is underpinned by a modular design philosophy, facilitating flexibility and scalability. The core components include:

\paragraph{Data Preprocessing Module} 
This initial module is responsible for standardizing and preparing the input data from stereo cameras and LiDAR sensors. It includes procedures for data transformation, cropping, augmentation, and the application of null masks, as detailed in the pre-processing section.

\paragraph{Fusion Core}
At the heart of our method lies the fusion core, which employs the adapted Papoulis-Gerchberg algorithm. This component integrates the pre-processed stereo and LiDAR data, leveraging iterative refinement to enhance the depth map's resolution and accuracy.

\paragraph{Post-Processing Module}
Following the fusion process, this module applies several refinement techniques, including the removal of trivial augmented data, smoothing, consistency checks, and edge sharpening, ensuring the output depth map is of the highest quality and ready for application use.

\subsection{Adopting the Algorithm to Linear Algebra}

The adaptation of our algorithm to the principles of linear algebra is critical for achieving the desired computational efficiency and scalability. This adaptation involves:

\paragraph{Matrix Representation of Data} 
All input data, including depth frames from stereo images and LiDAR point clouds, are represented as matrices. This format facilitates the application of linear transformations and operations, streamlining the fusion process.

\paragraph{Linear Transformations for Data Fusion}
The core of the data fusion process relies on linear transformations, which are applied to align, scale, and integrate the stereo and LiDAR data within a unified depth framework. These transformations are mathematically defined to ensure precision and efficiency.

\paragraph{Iterative Refinement via Linear Operations} 
The iterative refinement process within the Papoulis-Gerchberg algorithm is implemented through a series of linear operations. This includes the use of linear filters for smoothing and the application of linear equations to model the iterative update mechanism.

% Optimization Techniques: To further enhance performance, linear algebra-based optimization techniques are employed. These techniques are designed to reduce computational complexity, improve convergence speed, and ensure the algorithm can be executed in real-time on hardware with limited processing capabilities.

% Parallel Processing and Hardware Acceleration: The algorithm is optimized for parallel processing, taking advantage of hardware acceleration features available in modern computing platforms. This optimization is facilitated by the decomposition of linear algebra operations into sub-tasks that can be executed concurrently, significantly reducing processing time.

\section{Optimization}

Building upon the foundational linear algebraic principles outlined in the implementation section (\ref{implementation}), we have further optimized our extended Papoulis-Gerchberg (PG) method to harness parallel processing capabilities. This optimization enables our approach to achieve significant computational efficiency, making it competitive with contemporary sensor fusion methods that utilize deep neural networks (DNNs) and other computationally intensive techniques, often reliant on GPU acceleration.
(Buraya DNN gibi yapıları kullanarak GPU üzerinde koşan sensor fusion paperlarına alıntı yapılacak)

\subsection{GPU Acceleration}

The cornerstone of our optimization strategy is GPU acceleration, which is particularly well-suited to the linear algebra operations at the heart of our method. By translating our algorithm's core processes into GPU-optimized computations, we can leverage the massive parallelism offered by modern GPUs. This allows for the simultaneous processing of multiple data elements, drastically reducing the time required for depth map fusion and refinement.

\paragraph{Parallel Data Processing} Key operations, such as matrix multiplications and linear transformations, are implemented to take full advantage of GPU threads, ensuring that data from stereo images and LiDAR sensors can be processed concurrently.

\paragraph{Iterative Refinement Optimization} The iterative nature of the PG algorithm benefits immensely from GPU acceleration. Each iteration, involving the update and refinement of depth estimations, is executed across thousands of GPU cores, significantly speeding up convergence towards the optimal depth map.

\paragraph{Memory Management and Data Transfer} Efficient memory management techniques are employed to minimize the overhead associated with transferring data between CPU and GPU memory spaces. This includes the use of pinned memory and asynchronous data transfer operations to maintain high throughput and low latency.

\subsection{Cloud Computing}

To further enhance the scalability and accessibility of our sensor fusion method, we integrate cloud computing resources. This integration not only extends the computational capabilities beyond the confines of local hardware but also facilitates the processing of large datasets and the deployment of our solution in distributed systems.

\paragraph{Scalable Computing Resources} By leveraging cloud-based GPUs and distributed computing environments, our method can dynamically allocate resources based on the complexity of the data and the required processing power, ensuring efficient handling of varying workloads.

\paragraph{Collaborative Processing} The cloud computing model enables collaborative sensor fusion tasks, where data from multiple sources can be integrated and processed in a centralized manner. This is particularly advantageous for applications requiring real-time analysis of data from distributed sensor networks.

\paragraph{Deployment Flexibility} The adoption of cloud computing allows for the deployment of our sensor fusion solution across a range of platforms, from mobile devices with limited processing capabilities to enterprise-level systems requiring high throughput and real-time performance.

\section{Experiment}
To test the performance and quality of our work, we developed some test methods. Although there are widely used datasets to test the stereo and flow depth estimation algorithms, due to the fundamental difference between well known depth estimation algorithms and our work, we had to design custom tests and benchmarks.
The traditional depth estimation algorithms are mainly focuses on generating disparity maps using stereo pair or flow image pairs, the datasets and benchmarks provide stereo pairs as input data and LiDAR measurements as ground truth. Unlike a standard depth estimation method, our multi-sensory system uses also the LiDAR measurements to predict depth.
We designed different tests to both compare our method with traditional single sensory methods and test it with real life conditions.
\subsection{Experimenting our Algorithm With KITTI Benchmark Suite}
The KITTI Vision Benchmark Suite is a well-established resource for researchers and developers working on computer vision algorithms, particularly those related to autonomous driving. It provides a diverse set of datasets and evaluation tools for various tasks, including:

\paragraph{Stereo vision:} This involves reconstructing 3D information from two slightly different camera viewpoints. KITTI offers datasets for both static and dynamic scenarios.
\paragraph{Optical flow:} This measures the apparent motion of pixels between consecutive video frames, which is crucial for tasks like motion estimation and scene understanding.
\paragraph{Visual odometry:} This estimates the 3D motion of a camera solely based on visual information, which is essential for robot navigation.
3D object detection and tracking: This involves identifying and locating objects of interest (e.g., cars, pedestrians) in 3D space, while tracking their movement over time.
The suite leverages data captured from a specially equipped vehicle driving in various environments, including urban streets, rural areas, and highways. This data includes:

High-resolution color and grayscale camera images
Velodyne laser scanner data (LiDAR) for accurate depth information
GPS localization data
Researchers can use the KITTI datasets to train and evaluate their algorithms, and compare their performance against others on the provided leaderboards. This fosters advancements in computer vision techniques with real-world applications, particularly in the field of autonomous driving.
%% Mention the dataset preparation picking data from different datasets

\subsubsection{Effect of Shape and Format of LiDAR Data}
\subsubsection{Experiment Scenarios}

\subsection{Experiments With Our Sensor Suit}
%% Mention the moving system and challenges, calibration and synchronization
%% Realtime 

\subsubsection{Sensor Suit}
We conducted experiments using the data we collected from our sensors. We used Velodyne VLP-16 as 3D LiDAR and Zed Stereo Camera as main sensing devices. We also developed a suit that mounts the sensors we use for experimenting in fixed positions. The sensor suit has also an Nvidia Jetson Nano 4GB computer. The computer plays different roles in different experiment scenarios.
% put images of sensor suit only
To obtain real-life like conditions by mounting the sensor suit on top of a quadruped robot.
% put images of sensor suit with robot

\subsubsection{Experiment Scenarios}
We conducted different experiment scenarios to measure different metrics.
\paragraph{Cloud Computing (Performance):}
In  this scenario, the 
\paragraph{Mobile Computing (Performance):}
\paragraph{SLAM (Quality):}
% Put three slam maps and visual odometry visualization images {Only Stereo}, {Only Lidar}, {Fused}. AIM:Compare map quality and odomerty accuracy.


\section{Results}

\subsection{Results Obtained From KITTI Benchmark Suite Experiments}

\begin{table}[htbp]
    \caption{Evaluation of our method and standard mono-sensory methods}
    \begin{center}
        \begin{tabular}{|c|c|c|}
            \hline
            \textbf{Table}&\multicolumn{2}{|c|}{\textbf{Metrics}} \\
            \cline{2-3} 
            \textbf{Head} & \textbf{\textit{MAe}}& \textbf{\textit{RMSe}} \\
            \hline
            Stereo Depth& &   \\
            \hline
            LiDAR Measurement& More table copy$^{\mathrm{a}}$& \\
            \hline
            Ours& More table copy$^{\mathrm{a}}$& \\
            \hline
            \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
        \end{tabular}
        \label{tab1}
    \end{center}
\end{table}

\subsection{Results Obtained From Sensor Suit Experiments}

\begin{table}[htbp]
\caption{Table Type Styles}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{2-4} 
\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
\hline
copy& More table copy$^{\mathrm{a}}$& &  \\
\hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}


\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{refs} % Entries are in the refs.bib file

\vspace{12pt}
\color{red}
IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\vspace{12pt}
\color{red}
Tüm referanslar kontrol edilmeli (label referansları dahil)

\end{document}
