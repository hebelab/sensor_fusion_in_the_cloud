\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Enhancing Depth Estimation with Iterative Stereo-LiDAR Fusion: An Application of the Papoulis-Gerchberg Algorithm\\
{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
should not be used}
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{
\IEEEauthorblockN{1\textsuperscript{st} Assc. Prof. İsmail Uyanık}
\IEEEauthorblockA{\textit{Department of Electrical and Electronics Engineering} \\
\textit{Hacettepe University}\\
Ankara, Turkiye \\
uyanik@ee.hacettepe.edu.tr}
\and
\IEEEauthorblockN{2\textsuperscript{st} Ahmet Yusuf Şirin}
\IEEEauthorblockA{\textit{Department of Electrical and Electronics Engineering} \\
\textit{Hacettepe University}\\
Ankara, Turkiye \\
ayusufsirin@gmail.com}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Ertuğrul Tiyek}
\IEEEauthorblockA{\textit{Department of Electrical and Electronics Engineering} \\
\textit{Hacettepe University}\\
Ankara, Turkiye \\
ertugrultyk@gmail.com}
}

\maketitle

\begin{abstract}
Depth perception in autonomous systems is critically enhanced by sensor fusion techniques, which traditionally rely on the combination of stereo vision and LiDAR data. However, the challenge of integrating sparse LiDAR data with dense stereo imagery remains. This paper introduces a novel fusion approach that applies the Papoulis-Gerchberg algorithm to iteratively refine stereo depth maps with LiDAR data. Our method uses LiDAR measurements as anchor points to guide the enhancement of depth information, resulting in superior resolution and accuracy. We evaluate our approach using the KITTI benchmark suite, demonstrating significant improvements over existing methods such as FastFusion, particularly in edge definition and object boundary sharpness. The results suggest our method not only achieves higher fidelity in depth estimation but also offers a new perspective on sensor data integration for real-time applications in autonomous navigation.
\end{abstract}

\begin{IEEEkeywords}
Sensor Fusion, Stereo Vision, LiDAR Depth Sensing, Papoulis-Gerchberg Algorithm, Autonomous Vehicles, Iterative Enhancement, Depth Map, Super-Resolution, Real-Time Depth Estimation
\end{IEEEkeywords}

\section{Introduction}

The quest for accurate depth perception in autonomous systems has given rise to various sensor fusion methodologies, predominantly those employing neural networks. While these data-driven techniques have demonstrated remarkable performance, they often require extensive training on large datasets and substantial computational resources, limiting their practicality in dynamic real-world scenarios.

Contrary to the prevalent neural network-based approaches, this paper introduces a novel sensor fusion framework that does not rely on learning from data. By adapting the Papoulis-Gerchberg algorithm, traditionally used for signal reconstruction and super-resolution, our method iteratively refines stereo depth maps using LiDAR data without the need for training. This non-learning-based approach circumvents the complexities of neural models, resulting in a lightweight and versatile solution suitable for real-time applications.

Our methodology capitalizes on the complementary strengths of stereo vision and LiDAR, embedding accurate depth measurements into dense stereo maps to produce superior environmental representations. By eschewing neural networks, we avoid their inherent demand for computational power and data, presenting an approach that is not only more practical but also readily deployable in diverse operational contexts.

The following sections will outline the current landscape of depth sensing and fusion techniques, detail our fusion algorithm, and present a comparative analysis with existing neural network-based methods, showcasing the practical and performance benefits of our approach.

\section{Related Works}

Stereo matching and depth estimation are pivotal in applications such as autonomous driving and robotic navigation. Traditional methods, as outlined by Scharstein and Szeliski (2002), have evolved with the adoption of Deep Neural Network (DNN) techniques, significantly enhancing accuracy (Zbontar \& LeCun, 2016). However, the computational demands of these DNN solutions often outpace the capabilities of onboard systems in real-time scenarios (Shamsafar et al., 2022; Rahim et al., 2021).

In the realm of stereo-LiDAR fusion, approaches like those proposed by Parks et al. (2018) and Park et al. (2019) augment stereo disparity with LiDAR data but fail to fully exploit the precision of LiDAR. Choe et al. (2021) and Yang et al. (2019) introduce architectures that incorporate LiDAR data into early stages of depth estimation but often at a sacrifice of either depth integration or accuracy. Other methods, such as those by Mai et al. (2021) and Li and Cao (2020), achieve higher accuracy but at the cost of increased computational load, which is not conducive to real-time application.

Our work diverges from these DNN-heavy strategies by employing the Papoulis-Gerchberg algorithm, a technique historically utilized in signal processing for super-resolution, adapted here for sensor fusion. This approach circumvents the prohibitive computational demands of DNNs, offering a lightweight yet robust alternative for real-time, accurate depth estimation from stereo-LiDAR data fusion.

\section{Technical Approach}
%% In this section explain the each step of the processing detaily.

The Papoulis-Gerchberg (PG) algorithm is an iterative technique employed in signal processing to perform band-limited signal extrapolation. This method enables the estimation of missing portions of a signal based on a limited segment of the original data and the knowledge of its bandwidth.

The PG algorithm operates through an iterative process involving two key steps:

\begin{enumerate}
    \item Constraint Enforcement in the Spatial Domain:
    The initial step involves enforcing the known samples of the signal. This is achieved by transforming the estimated spectrum (obtained through techniques like the Fourier transform) of the signal and modifying it to ensure it aligns with the known Fourier Transform values corresponding to the available samples.
    
    \item Constraint Enforcement in the Frequency Domain:
    The second step focuses on enforcing the known bandwidth constraint. This is implemented by modifying the signal's estimated version in the spatial domain (e.g., time domain for time-based signals) to guarantee it adheres to the specified bandwidth limitations.
\end{enumerate}
These two steps are repeated iteratively. As each iteration progresses, the estimated signal progressively converges towards the original signal, ultimately resulting in an extrapolated version that replenishes the missing data points.

Further works opened the papoulis-gerchberg algorithm new using areas like signal restoration and image processing.
%% Our approach is based on the PG method that is commonly used for estimation and reconstruction of lossy signals in communications systems. The method is applicable for 1D signals, originally. The 2D implementation of it is required for us. There are some works on implementation to 2D of PG. 

%% Paper about signal restoration
%% Paper about super resolution
Another application of this algorithm is used for image inpainting by \cite{pg-image-inpaint-2009}. They introduce the PG for this specific scenerio but it has some precautions to use for us. The images they used are extremely dense when comparing with the Lidar. Also, they have single sensory image but we need to optain a fusion. 

Another work done by \cite{ozbay2015high} that introduce an extended version of PG for 3D point cloud data that is driven by a Lidar sensor. They use this as an upsampling to cope with delayed points when using Lidar in high resolution mode due to the low scan rates of cylindirical scanning ones. But appearantly, it also use single sensory system

We introduce our novel approach my putting another extension of PG to the litarature that is the first one uses multi-sensory system. The nature of PG consists some elements. 

%% PG açıklaması başta olsa daha iyi olabilir.

\subsection{Pre-Processing}
\subsubsection{Data Augmentation}
Due to the fundamental difference in the representation of depth information between liDAR measurements (point cloud) and stereo depth estimation (dense disparity maps), both data modalities necessitate preprocessing steps involving transformation, conversion, and upsampling to facilitate their integration and subsequent processing.

REVISE - To make these two sensor data processable in a domain, we determine a common modality for these images. This modality is single channel depth image format. The RGB integration wont necessite for using our algorithm.

Stereo depth augmentation requirement is originated from the deficit nature of the disparity maps that are used in depth estimation from stereo cameras. The depth images, due to the disparity maps, have some null values at blind spots that one of the cameras have no view at all. These null values at the depth images should be pre-processed before the PG algorithm to avoid the distortion of the well estimated depth entities (pixels).

We proposed a method to overcome the null value problem. This method fills the gaps (null values at the depth images) by using the lidar measurements. Firstly, a segmentation of the gaps are made and then the lidar measurements are taken into account to fill the gap segments that are overlapped with lidar measurements. Again, due to the sparse nature of the lidar measurements, we made a interpolation for filling the huge gaps by using lidar measurements that have small area. 


\paragraph{Obtaining Depth Frame From Lidar Data}

After making the calibrations for stereo camera and the Lidar, we know all the transformation matrices to generate the depth image from point cloud data. To project the 3D points into the 2D depth frame space we use the pinhole camera model. 
%% TODO: Pinhole camera model daha mu uygun? use the intrinsic parameters and the known transformation matrices.

\paragraph{Lidar Null Mask}

The lidar depth frames will have null values do to the nature of the projection of sparse point cloud data into a plane. This null values should be considered before using the lidar depth map in the algorithm.

Since, our method use the stereo depth maps as a starting point and lidar depth map as rectifier of depth information of these stereo depth images, we need a mask of null values to eliminate the spurious pixels (depth entities) at the post-processing part of our algorithm.

\paragraph{Stereo Infill}

\subsection{PG: Papoulis GerchBerg}
%% \subsubsection{PG Background}
%% Cite other papers about papoulis gerchberg, super resolution

\subsubsection{Our Approach}
\subsubsection{Use of Band Reject Filter in PG}
\subsubsection{Evaluated PG for Our Work}
\subsection{Post-Proc}
\subsubsection{Removal of Trivial Aumented Data}

\section{Implementation} \label{implementation}
\subsection{Adopting the Algorithm to Linear Algebra}

\section{Optimization}

As defined in the previous section \ref{implementation}, our method is developed baed on the linear algebraic methods. Using the nature of the linear algebra, we introduce a parallel processing approach for our extended PG method. This introduce us a workspace that we can competite with our fellows (Buraya DNN gibi yapıları kullanarak GPU üzerinde koşan sensor fusion paperlarına alıntı yapılacak)

\subsection{GPU Acceleration}
\subsection{Cloud Computing}

\section{Experiment}

\subsection{Experimenting our Algorithm With Kitti Benchmark Suite}
%% Mention the dataset preparation picking data from different datasets

\subsubsection{Effect of Shape and Format of Lidar Data}
\subsubsection{Experiment Scenarios}

\subsection{Experiments With Our Sensor Suit}
%% Mention the moving system and challenges, calibration and synchronization
%% Realtime 

\subsubsection{Sensor Suit}
\paragraph{Upsampling of Lidar Data}
%% Further improvements dynamic lidar upsampling with kalman

\subsubsection{Experiment Scenarios}


\section{Results}

\subsection{Abbreviations and Acronyms}\label{AA}


\subsection{Units}
\begin{itemize}
\item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
\item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
\item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
\item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
\end{itemize}

\subsection{Equations}
Number equations consecutively. To make your 
equations more compact, you may use the solidus (~/~), the exp function, or 
appropriate exponents. Italicize Roman symbols for quantities and variables, 
but not Greek symbols. Use a long dash rather than a hyphen for a minus 
sign. Punctuate equations with commas or periods when they are part of a 
sentence, as in:
\begin{equation}
a+b=\gamma\label{eq}
\end{equation}

Be sure that the 
symbols in your equation have been defined before or immediately following 
the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
the beginning of a sentence: ``Equation \eqref{eq} is . . .''

\subsection{\LaTeX-Specific Advice}

Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
of ``hard'' references (e.g., \verb|(1)|). That will make it possible
to combine sections, add equations, or change the order of figures or
citations without having to go through the file line by line.

Please don't use the \verb|{eqnarray}| equation environment. Use
\verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
environment leaves unsightly spaces around relation symbols.

Please note that the \verb|{subequations}| environment in {\LaTeX}
will increment the main equation counter even when there are no
equation numbers displayed. If you forget that, you might write an
article in which the equation numbers skip from (17) to (20), causing
the copy editors to wonder if you've discovered a new method of
counting.

{\BibTeX} does not work by magic. It doesn't get the bibliographic
data from thin air but from .bib files. If you use {\BibTeX} to produce a
bibliography you must send the .bib files. 

{\LaTeX} can't read your mind. If you assign the same label to a
subsubsection and a table, you might find that Table I has been cross
referenced as Table IV-B3. 

{\LaTeX} does not have precognitive abilities. If you put a
\verb|\label| command before the command that updates the counter it's
supposed to be using, the label will pick up the last counter to be
cross referenced instead. In particular, a \verb|\label| command
should not go before the caption of a figure or a table.

Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
will not stop equation numbers inside \verb|{array}| (there won't be
any anyway) and it might stop a wanted equation number in the
surrounding equation.

\subsection{Some Common Mistakes}\label{SCM}
\begin{itemize}
\item The word ``data'' is plural, not singular.
\item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
\item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
\item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
\item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
\item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
\item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
\item Do not confuse ``imply'' and ``infer''.
\item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
\item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
\item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
\end{itemize}
An excellent style manual for science writers is \cite{b7}.

\subsection{Authors and Affiliations}
\textbf{The class file is designed for, but not limited to, six authors.} A 
minimum of one author is required for all conference articles. Author names 
should be listed starting from left to right and then moving down to the 
next line. This is the author sequence that will be used in future citations 
and by indexing services. Names should not be listed in columns nor group by 
affiliation. Please keep your affiliations as succinct as possible (for 
example, do not differentiate among departments of the same organization).

\subsection{Identify the Headings}
Headings, or heads, are organizational devices that guide the reader through 
your paper. There are two types: component heads and text heads.

Component heads identify the different components of your paper and are not 
topically subordinate to each other. Examples include Acknowledgments and 
References and, for these, the correct style to use is ``Heading 5''. Use 
``figure caption'' for your Figure captions, and ``table head'' for your 
table title. Run-in heads, such as ``Abstract'', will require you to apply a 
style (in this case, italic) in addition to the style provided by the drop 
down menu to differentiate the head from the text.

Text heads organize the topics on a relational, hierarchical basis. For 
example, the paper title is the primary text head because all subsequent 
material relates and elaborates on this one topic. If there are two or more 
sub-topics, the next level head (uppercase Roman numerals) should be used 
and, conversely, if there are not at least two sub-topics, then no subheads 
should be introduced.

\subsection{Figures and Tables}
\paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
bottom of columns. Avoid placing them in the middle of columns. Large 
figures and tables may span across both columns. Figure captions should be 
below the figures; table heads should appear above the tables. Insert 
figures and tables after they are cited in the text. Use the abbreviation 
``Fig.~\ref{fig}'', even at the beginning of a sentence.

\begin{table}[htbp]
\caption{Table Type Styles}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{2-4} 
\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
\hline
copy& More table copy$^{\mathrm{a}}$& &  \\
\hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics{fig1.png}}
\caption{Example of a figure caption.}
\label{fig}
\end{figure}

Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
rather than symbols or abbreviations when writing Figure axis labels to 
avoid confusing the reader. As an example, write the quantity 
``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
units in the label, present them within parentheses. Do not label axes only 
with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
\{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
quantities and units. For example, write ``Temperature (K)'', not 
``Temperature/K''.

\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.

\section*{References}

Please number citations consecutively within brackets \cite{b1}. The 
sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

Number footnotes separately in superscripts. Place the actual footnote at 
the bottom of the column in which it was cited. Do not put footnotes in the 
abstract or reference list. Use letters for table footnotes.

Unless there are six authors or more give all authors' names; do not use 
``et al.''. Papers that have not been published, even if they have been 
submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
Capitalize only the first word in a paper title, except for proper nouns and 
element symbols.

For papers published in translation journals, please give the English 
citation first, followed by the original foreign-language citation \cite{b6}.

\begin{thebibliography}{00}
\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
\end{thebibliography}

\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{refs} % Entries are in the refs.bib file

\vspace{12pt}
\color{red}
IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
