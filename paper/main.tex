\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Enhancing Depth Estimation with Iterative Stereo-LiDAR Fusion: An Application of the Papoulis-Gerchberg Algorithm\\
{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
should not be used}
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{
\IEEEauthorblockN{1\textsuperscript{st} Assc. Prof. İsmail Uyanık}
\IEEEauthorblockA{\textit{Department of Electrical and Electronics Engineering} \\
\textit{Hacettepe University}\\
Ankara, Turkiye \\
uyanik@ee.hacettepe.edu.tr}
\and
\IEEEauthorblockN{2\textsuperscript{st} Ahmet Yusuf Şirin}
\IEEEauthorblockA{\textit{Department of Electrical and Electronics Engineering} \\
\textit{Hacettepe University}\\
Ankara, Turkiye \\
ayusufsirin@gmail.com}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Ertuğrul Tiyek}
\IEEEauthorblockA{\textit{Department of Electrical and Electronics Engineering} \\
\textit{Hacettepe University}\\
Ankara, Turkiye \\
ertugrultyk@gmail.com}
}

\maketitle

\begin{abstract}
Depth perception in autonomous systems is critically enhanced by sensor fusion techniques, which traditionally rely on the combination of stereo vision and LiDAR data. However, the challenge of integrating sparse LiDAR data with dense stereo imagery remains. This paper introduces a novel fusion approach that applies the Papoulis-Gerchberg algorithm to iteratively refine stereo depth maps with LiDAR data. Our method uses LiDAR measurements as anchor points to guide the enhancement of depth information, resulting in superior resolution and accuracy. We evaluate our approach using the KITTI benchmark suite, demonstrating significant improvements over existing methods such as FastFusion, particularly in edge definition and object boundary sharpness. The results suggest our method not only achieves higher fidelity in depth estimation but also offers a new perspective on sensor data integration for real-time applications in autonomous navigation.
\end{abstract}

\begin{IEEEkeywords}
Sensor Fusion, Stereo Vision, LiDAR Depth Sensing, Papoulis-Gerchberg Algorithm, Autonomous Vehicles, Iterative Enhancement, Depth Map, Super-Resolution, Real-Time Depth Estimation
\end{IEEEkeywords}

\section{Introduction}

The quest for accurate depth perception in autonomous systems has given rise to various sensor fusion methodologies, predominantly those employing neural networks. While these data-driven techniques have demonstrated remarkable performance, they often require extensive training on large datasets and substantial computational resources, limiting their practicality in dynamic real-world scenarios.

Contrary to the prevalent neural network-based approaches, this paper introduces a novel sensor fusion framework that does not rely on learning from data. By adapting the Papoulis-Gerchberg algorithm, traditionally used for signal reconstruction and super-resolution, our method iteratively refines stereo depth maps using LiDAR data without the need for training. This non-learning-based approach circumvents the complexities of neural models, resulting in a lightweight and versatile solution suitable for real-time applications.

Our methodology capitalizes on the complementary strengths of stereo vision and LiDAR, embedding accurate depth measurements into dense stereo maps to produce superior environmental representations. By eschewing neural networks, we avoid their inherent demand for computational power and data, presenting an approach that is not only more practical but also readily deployable in diverse operational contexts.

The following sections will outline the current landscape of depth sensing and fusion techniques, detail our fusion algorithm, and present a comparative analysis with existing neural network-based methods, showcasing the practical and performance benefits of our approach.

\section{Related Works}

Stereo matching and depth estimation are pivotal in applications such as autonomous driving and robotic navigation. Traditional methods, as outlined by Scharstein and Szeliski (2002), have evolved with the adoption of Deep Neural Network (DNN) techniques, significantly enhancing accuracy (Zbontar \& LeCun, 2016). However, the computational demands of these DNN solutions often outpace the capabilities of onboard systems in real-time scenarios (Shamsafar et al., 2022; Rahim et al., 2021).

In the realm of stereo-LiDAR fusion, approaches like those proposed by Parks et al. (2018) and Park et al. (2019) augment stereo disparity with LiDAR data but fail to fully exploit the precision of LiDAR. Choe et al. (2021) and Yang et al. (2019) introduce architectures that incorporate LiDAR data into early stages of depth estimation but often at a sacrifice of either depth integration or accuracy. Other methods, such as those by Mai et al. (2021) and Li and Cao (2020), achieve higher accuracy but at the cost of increased computational load, which is not conducive to real-time application.

Our work diverges from these DNN-heavy strategies by employing the Papoulis-Gerchberg algorithm, a technique historically utilized in signal processing for super-resolution, adapted here for sensor fusion. This approach circumvents the prohibitive computational demands of DNNs, offering a lightweight yet robust alternative for real-time, accurate depth estimation from stereo-LiDAR data fusion.

\section{Technical Approach}
%% In this section explain the each step of the processing detaily.

The Papoulis-Gerchberg (PG) algorithm is an iterative technique employed in signal processing to perform band-limited signal extrapolation. This method enables the estimation of missing portions of a signal based on a limited segment of the original data and the knowledge of its bandwidth.

The PG algorithm operates through an iterative process involving two key steps:

\begin{enumerate}
    \item Constraint Enforcement in the Spatial Domain:
    The initial step involves enforcing the known samples of the signal. This is achieved by transforming the estimated spectrum (obtained through techniques like the Fourier transform) of the signal and modifying it to ensure it aligns with the known Fourier Transform values corresponding to the available samples.
    
    \item Constraint Enforcement in the Frequency Domain:
    The second step focuses on enforcing the known bandwidth constraint. This is implemented by modifying the signal's estimated version in the spatial domain (e.g., time domain for time-based signals) to guarantee it adheres to the specified bandwidth limitations.
\end{enumerate}

These two steps are repeated iteratively. As each iteration progresses, the estimated signal progressively converges towards the original signal, ultimately resulting in an extrapolated version that replenishes the missing data points.

Further works opened the papoulis-gerchberg algorithm new using areas like signal restoration and image processing.
%% Our approach is based on the PG method that is commonly used for estimation and reconstruction of lossy signals in communications systems. The method is applicable for 1D signals, originally. The 2D implementation of it is required for us. There are some works on implementation to 2D of PG. 

%% Paper about signal restoration
%% Paper about super resolution
An application of this algorithm for image inpainting was proposed by \cite{pg-image-inpaint-2009}, wherein the Papoulis-Gerchberg (PG) algorithm is adapted for this particular scenario. However, there are several caveats to its application within our context. Primarily, the images utilized in their study exhibit significantly higher density compared to LiDAR data. Furthermore, their methodology is predicated upon the utilization of a singular sensory modality, whereas our objective necessitates the achievement of fusion across multiple sensors.

Another noteworthy contribution is made by \cite{ozbay2015high}, presenting an enhanced iteration of the PG algorithm tailored for 3D point cloud data derived from LiDAR sensors. Their method focuses on upsampling to mitigate the latency in point acquisition observed when operating LiDAR at high-resolution modes, attributable to the inherently slow scan rates of cylindrical scanning systems. Nevertheless, it is important to note that this approach, too, relies on a singular sensory system.

The utilization of the Papoulis-Gerchberg (PG) algorithm within our framework is primarily motivated by its deterministic nature, setting it apart from machine learning-based methodologies that often rely on probabilistic outcomes. A closer examination of the PG algorithm's underlying principles reveals two pivotal components: a densely populated base image intended for gap-filling using points of known accuracy, and the employment of these high-confidence points for iterative refinement. This paradigm bears a striking resemblance to the challenges inherent in sensor fusion. In our methodology, stereo depth images serve as the densely populated base analogous to the PG algorithm's framework, while LiDAR sparse data represent the high-confidence points. This analogy forms the cornerstone of our approach, leveraging the PG algorithm's architecture to integrate multi-sensorial data, thereby pioneering a novel application of PG in the realm of sensor fusion.

\subsection{Pre-Processing}

The fusion of data from heterogeneous sensor systems, such as LiDAR and stereo cameras, presents a unique set of challenges, primarily due to the fundamental differences in their respective depth information representations. LiDAR sensors provide sparse but highly accurate point clouds, whereas stereo vision techniques yield dense disparity maps with varying degrees of precision. To effectively integrate these diverse data modalities, a comprehensive pre-processing regimen is indispensable. This regimen encompasses transformation, conversion, and upsampling procedures designed to standardize the data, thereby laying the groundwork for their combined analysis.

Transformation and Conversion: The initial step involves transforming the LiDAR-generated point clouds and stereo vision-produced disparity maps into a uniform format. We opt for the single-channel depth image format as this common ground facilitates the merging process. This transformation is critical not only for aligning the data spatially but also for ensuring that subsequent processing stages operate on data that are consistent in terms of dimensionality and scale.

Upsampling: Given the sparse nature of LiDAR data compared to the relatively denser outputs from stereo vision, upsampling is employed to increase the resolution of the LiDAR measurements. This step is crucial for achieving a density comparable to that of the stereo disparity maps, thereby enhancing the reliability of the fused depth information. Upsampling is carefully executed to preserve the integrity of the LiDAR data, ensuring that the enhanced resolution does not compromise the accuracy inherent to LiDAR measurements.

RGB Integration Exclusion: It is worth noting that our approach does not necessitate the integration of RGB data. This decision streamlines the preprocessing phase, focusing solely on depth information, which is paramount for our application. The exclusion of RGB data underscores our methodology's emphasis on depth accuracy and computational efficiency, aligning with the objective of developing a lightweight and practical fusion algorithm.

By meticulously addressing these preprocessing steps, we ensure that the LiDAR and stereo depth data are optimally prepared for fusion. This preparation is vital for the successful application of our novel algorithm, which leverages the strengths of both sensor types to produce depth estimations of unparalleled accuracy and density.

\subsubsection{Obtaining Depth Frame From LiDAR Data}

Subsequent to the calibration of the stereo camera and LiDAR sensor, the transformation matrices essential for converting point cloud data into a depth image are established. The conversion employs the pinhole camera model to accurately project three-dimensional points onto a two-dimensional depth frame space. This process is fundamental in translating the spatial coordinates of LiDAR measurements into a format that is compatible with stereo vision data, thereby facilitating their integration.

\subsubsection{Data Cropping}

Projection of LiDAR data onto the stereo depth image plane reveals discrepancies in the boundary extents of the depth regions. This is particularly evident in the vertical scan range of the LiDAR sensor, which is relatively narrow (approximately 30 degrees) compared to the broader field of view captured by the stereo camera. Consequently, it becomes necessary to crop the stereo depth image to match the LiDAR's scan area. Moreover, the stereo depth image exhibits a narrower horizontal boundary compared to the LiDAR's 360-degree scanning capability. Given our algorithm's reliance on depth images and the absence of memory for previously detected points in frame sensor fusion, non-overlapping depth data regions are disregarded to maintain algorithmic integrity and focus on the fusion of congruent depth information.

\subsubsection{Data Augmentation (Stereo Infill)}

The augmentation of stereo depth data addresses the challenge posed by inherent limitations in stereo vision disparity maps, which frequently exhibit null values in occluded regions not visible to one or both of the stereo cameras. These occlusions result in incomplete depth information, manifesting as gaps in the depth images derived from stereo vision. To rectify this and ensure a comprehensive depth field, our method incorporates a strategic process of gap filling that leverages high-confidence LiDAR measurements.

Gap Segmentation and Identification: The initial phase involves a meticulous analysis of the stereo depth images to segment and identify areas devoid of depth information—these are the regions where disparity maps fail to provide reliable depth due to occlusions or insufficient texture.

LiDAR Measurement Integration: Upon identifying these gaps, the process proceeds by overlaying LiDAR data onto the stereo depth images. This step utilizes the precise depth information from LiDAR measurements to fill in the identified gaps, prioritizing regions where LiDAR data overlaps with the gaps in stereo images. This integration capitalizes on the accuracy and reliability of LiDAR measurements to enhance the fidelity of the depth information in areas previously marked by uncertainty.

Interpolation for Comprehensive Coverage: Recognizing the sparse nature of LiDAR data and its inability to cover all gaps fully, we employ interpolation techniques as a subsequent measure. This approach extrapolates the available LiDAR measurements to fill larger gaps, ensuring that even areas without direct LiDAR coverage receive enhanced depth estimations. The interpolation considers the spatial distribution of LiDAR points and the surrounding depth context to generate a coherent depth field that bridges the disparity-induced gaps with plausible depth values.

This augmented approach to stereo depth infill not only rectifies the limitations inherent in stereo disparity maps but also enhances the overall depth accuracy of the fused stereo-LiDAR image. By meticulously integrating and interpolating LiDAR data within the stereo depth framework, we achieve a more robust and complete representation of the scene's depth, paving the way for improved performance in subsequent processing stages of our sensor fusion algorithm.

This phase represents a pivotal component of our preprocessing strategy, primarily due to its critical role in safeguarding the integrity of the Papoulis-Gerchberg (PG) algorithm's iterative process. In the absence of this meticulous data augmentation step, the iterative nature of the PG algorithm could inadvertently exacerbate depth estimation errors with each subsequent iteration. Specifically, unaddressed gaps in the stereo depth images could lead to erroneous propagation and amplification of inaccuracies, undermining the algorithm's ability to refine and converge towards a precise depth map. Therefore, by ensuring that these gaps are effectively filled through the integration and interpolation of LiDAR data, we substantially enhance the stability and accuracy of the depth estimation process, particularly as the number of iterations increases. This augmentation is not merely a preparatory measure but a fundamental prerequisite for harnessing the full potential of the PG algorithm in achieving high-fidelity depth estimations from fused stereo-LiDAR data.

\subsubsection{LiDAR Null Mask}

The projection of sparse LiDAR point cloud data onto a two-dimensional plane inherently results in the presence of null values within the depth frames. These null values must be meticulously accounted for prior to the integration of LiDAR depth maps into our fusion algorithm. To address this, we employ a null value mask for the LiDAR depth data, which serves to identify and subsequently exclude spurious depth entities from the fused depth estimation. This preprocessing step is crucial for refining the depth information derived from stereo depth maps, utilizing the LiDAR data as a corrective measure to enhance the fidelity of the resultant depth image.

\subsection{PG: Papoulis Gerchberg}
We first intended to use Papoulish-Gerchberg algorithm as a pre-processing method to up sample the only LiDAR data. But the sparsity of the LiDAR data made it challenging to work with.

We tried to apply zero-padding to empty regions of the depth frame obtained by the LiDAR samples. Since the Papoulish-Gerchberg method is an iterative method, zero background filling caused high amount of iterations to get reasonable results. And due to the sparse data especially in the vertical axis, the regions far from original LiDAR samples never get good results like the closer ones.
%% add an image to represent the papoulis with zero-padding

Then to decrease number of iterations, we tried to initialize the background with mean of actual LiDAR samples. Since the mean value is much closer to the actual depth of the neighboring samples, it significantly reduced the iterations. But the problem with the sparsity of LiDAR data was still present.
%% add another figure for mean filling

We need more significant data to overcome this problem. The initial solution was the using previous work of our colleague @ismail-uyanik, {Enhancing 3D range image measurement density via dynamic Papoulis-Gerchberg algorithm} along with the mean filling method.
%% cite the paper from kuzucu-elvan
%% add another image explaining the plan
\subsubsection{Our Approach}
As the purpose of this study is to investigate the potential of integrating data obtained from stereo cameras and LiDAR measurements, we also have the depth information from stereo image pair. Our motivation of this paper's work turned to merge both processes, up sampling of LiDAR data and fusing the depth information obtained from both sources. This method extremely decreased the need of processing power, and allowed real-time application of the sensor fusion even in mobile computing devices.
Basically the method is nothing but proposing the background of lidar measurements as depth frame from stereo pair instead of using only a mean value. Since the background filling is not non-significant data, we have mostly overcame the problem with sparsity of LiDAR measurements.
We also made some modifications and improvements to the standard 2D Papoulish-Gerchberg algorithm to match our needs better.
\paragraph{Use of Band Reject Filter in PG}
As the nature of the low pass filters used in papoulis-gerchberg algorithm, they all eliminate the high frequency components as they meant to be. But in our case, our data also need to be having high frequency components 
\paragraph{Evaluated PG for Our Work}

\subsection{Post-Proc}

The post-processing consinst of two major parts 

\subsubsection{Removal of Trivial Augmented Data}

\section{Implementation} \label{implementation}

The implementation of a proposed method is essential for experiments. Because of the robotic fields requirements, there are some constraints about the frequency of the algorithm, data rate of te sensors, and some other important facts. To accomplish all of this constraints, we use the power of the mathematics and the linear algebra. 

\subsection{Building the Algorithm}

\subsection{Adopting the Algorithm to Linear Algebra}

\section{Optimization}

As defined in the previous section \ref{implementation}, our method is developed baed on the linear algebraic methods. Using the nature of the linear algebra, we introduce a parallel processing approach for our extended PG method. This introduce us a workspace that we can competite with our fellows (Buraya DNN gibi yapıları kullanarak GPU üzerinde koşan sensor fusion paperlarına alıntı yapılacak)

\subsection{GPU Acceleration}
\subsection{Cloud Computing}

\section{Experiment}

\subsection{Experimenting our Algorithm With Kitti Benchmark Suite}
%% Mention the dataset preparation picking data from different datasets

\subsubsection{Effect of Shape and Format of LiDAR Data}
\subsubsection{Experiment Scenarios}

\subsection{Experiments With Our Sensor Suit}
%% Mention the moving system and challenges, calibration and synchronization
%% Realtime 

\subsubsection{Sensor Suit}
\paragraph{Upsampling of LiDAR Data}
%% Further improvements dynamic LiDAR upsampling with kalman

\subsubsection{Experiment Scenarios}


\section{Results}


\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{refs} % Entries are in the refs.bib file

\vspace{12pt}
\color{red}
IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
